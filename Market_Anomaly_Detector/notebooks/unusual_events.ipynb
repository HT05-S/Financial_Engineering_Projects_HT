{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92f21f27-9a96-4c8c-ad6a-26b866766535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Try to import yfinance with error handling\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    YFINANCE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    YFINANCE_AVAILABLE = False\n",
    "    st.error(\"yfinance is not installed. Please install it using: pip install yfinance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b0b0261-2349-4d5a-aad8-692f035355c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Advanced Market Anomaly Detection System\n",
    "    \n",
    "    This class implements multiple statistical and machine learning methods\n",
    "    for detecting unusual events in financial time series data.\n",
    "    \n",
    "    Methods include:\n",
    "    1. Statistical Process Control (SPC) using control charts\n",
    "    2. Z-Score based outlier detection with rolling statistics\n",
    "    3. Isolation Forest for multivariate anomaly detection\n",
    "    4. Volatility clustering detection using GARCH-like approach\n",
    "    5. Jump detection using Bipower Variation\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "351b3fdd-ede2-4166-a22e-9809f6d3230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the detector with price data\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): DataFrame with OHLCV data and datetime index\n",
    "        \"\"\"\n",
    "        if data is None or data.empty:\n",
    "            raise ValueError(\"Input data is empty or None\")\n",
    "            \n",
    "        self.data = data.copy()\n",
    "        \n",
    "        # Ensure we have the required columns\n",
    "        if 'Close' not in self.data.columns:\n",
    "            # Try different column names\n",
    "            if 'close' in self.data.columns:\n",
    "                self.data['Close'] = self.data['close']\n",
    "            elif 'Adj Close' in self.data.columns:\n",
    "                self.data['Close'] = self.data['Adj Close']\n",
    "            else:\n",
    "                raise ValueError(\"No 'Close' price column found in data\")\n",
    "        \n",
    "        self.returns = None\n",
    "        self.features = None\n",
    "        self.anomalies = {}\n",
    "        self._prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f43ad052-0924-462c-9c67-30850974cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepare financial features for analysis\n",
    "        \n",
    "        Theory: Financial returns are more stationary than prices and exhibit\n",
    "        properties like volatility clustering, fat tails, and mean reversion.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate returns\n",
    "            self.data['Returns'] = self.data['Close'].pct_change()\n",
    "            \n",
    "            # Log returns (more theoretically sound for continuous compounding)\n",
    "            self.data['Log_Returns'] = np.log(self.data['Close'] / self.data['Close'].shift(1))\n",
    "            \n",
    "            # Realized volatility (rolling standard deviation)\n",
    "            self.data['Volatility'] = self.data['Returns'].rolling(window=20, min_periods=1).std()\n",
    "            \n",
    "            # Volume-weighted returns (if volume available)\n",
    "            if 'Volume' in self.data.columns and not self.data['Volume'].isna().all():\n",
    "                # Add small constant to avoid log(0)\n",
    "                volume_safe = self.data['Volume'].replace(0, 1)\n",
    "                self.data['Volume_Weighted_Returns'] = self.data['Returns'] * np.log(volume_safe)\n",
    "            \n",
    "            # Higher moments for fat-tail analysis\n",
    "            self.data['Returns_Squared'] = self.data['Returns'] ** 2\n",
    "            self.data['Returns_Cubed'] = self.data['Returns'] ** 3\n",
    "            \n",
    "            # Rolling skewness and kurtosis (20-day window)\n",
    "            self.data['Rolling_Skewness'] = self.data['Returns'].rolling(window=20, min_periods=5).skew()\n",
    "            self.data['Rolling_Kurtosis'] = self.data['Returns'].rolling(window=20, min_periods=5).kurt()\n",
    "            \n",
    "            # Price momentum indicators\n",
    "            self.data['RSI'] = self._calculate_rsi(self.data['Close'], window=14)\n",
    "            \n",
    "            # Drop NaN values but keep at least some data\n",
    "            initial_length = len(self.data)\n",
    "            self.data = self.data.dropna()\n",
    "            \n",
    "            if len(self.data) < 30:  # Minimum required for analysis\n",
    "                raise ValueError(f\"Insufficient data after cleaning. Only {len(self.data)} valid observations remaining from {initial_length}\")\n",
    "            \n",
    "            self.returns = self.data['Returns'].values\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error in data preparation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9e7c987-06f9-408c-8cc8-9fa3e3fe0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_rsi(self, prices, window=14):\n",
    "        \"\"\"\n",
    "        Calculate Relative Strength Index\n",
    "        \n",
    "        Theory: RSI measures momentum and can indicate overbought/oversold conditions\n",
    "        \"\"\"\n",
    "        try:\n",
    "            delta = prices.diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            rs = gain / (loss + 1e-10)\n",
    "            return 100 - (100 / (1 + rs))\n",
    "        except:\n",
    "            return pd.Series(50, index=prices.index)  # Return neutral RSI if calculation fails\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1694f237-0a5e-476d-a95c-74a3cd4f8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    " def detect_statistical_outliers(self, window=60, threshold=3):\n",
    "        \"\"\"\n",
    "        Statistical Process Control using rolling Z-scores\n",
    "        \n",
    "        Theory: Assumes returns follow a normal distribution locally.\n",
    "        Points beyond threshold standard deviations are flagged as anomalies.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Adjust window size if data is too small\n",
    "            window = min(window, len(self.data) // 2)\n",
    "            window = max(window, 10)  # Minimum window size\n",
    "            \n",
    "            # Rolling mean and standard deviation\n",
    "            rolling_mean = self.data['Returns'].rolling(window=window, min_periods=window//2).mean()\n",
    "            rolling_std = self.data['Returns'].rolling(window=window, min_periods=window//2).std()\n",
    "            \n",
    "            # Calculate Z-scores, handle division by zero\n",
    "            z_scores = (self.data['Returns'] - rolling_mean) / (rolling_std + 1e-10)\n",
    "            \n",
    "            # Detect anomalies\n",
    "            anomalies = np.abs(z_scores) > threshold\n",
    "            \n",
    "            self.anomalies['statistical'] = {\n",
    "                'anomalies': anomalies,\n",
    "                'z_scores': z_scores,\n",
    "                'threshold': threshold\n",
    "            }\n",
    "            \n",
    "            return anomalies\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.warning(f\"Error in statistical outlier detection: {str(e)}\")\n",
    "            return pd.Series(False, index=self.data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a59c728-9b31-4499-a7e5-cbbe1c2c4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_volatility_clusters(self, window=20, threshold=2):\n",
    "        \"\"\"\n",
    "        Volatility Clustering Detection\n",
    "        \n",
    "        Theory: Financial markets exhibit volatility clustering - periods of high\n",
    "        volatility tend to be followed by high volatility (ARCH/GARCH effects).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Adjust window size\n",
    "            window = min(window, len(self.data) // 3)\n",
    "            window = max(window, 5)\n",
    "            \n",
    "            # Calculate rolling volatility statistics\n",
    "            vol_mean = self.data['Volatility'].rolling(window=window, min_periods=window//2).mean()\n",
    "            vol_std = self.data['Volatility'].rolling(window=window, min_periods=window//2).std()\n",
    "            \n",
    "            # Standardize current volatility\n",
    "            vol_z_score = (self.data['Volatility'] - vol_mean) / (vol_std + 1e-10)\n",
    "            \n",
    "            # Detect volatility spikes\n",
    "            vol_anomalies = vol_z_score > threshold\n",
    "            \n",
    "            self.anomalies['volatility'] = {\n",
    "                'anomalies': vol_anomalies,\n",
    "                'z_scores': vol_z_score,\n",
    "                'threshold': threshold\n",
    "            }\n",
    "            \n",
    "            return vol_anomalies\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.warning(f\"Error in volatility cluster detection: {str(e)}\")\n",
    "            return pd.Series(False, index=self.data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85065c3e-3a65-400a-9173-7663ee839a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def detect_jump_diffusion(self, window=20, significance_level=0.01):\n",
    "        \"\"\"\n",
    "        Jump Detection using Bipower Variation\n",
    "        \n",
    "        Theory: Distinguishes between continuous price movements (diffusion)\n",
    "        and discontinuous jumps. Based on Barndorff-Nielsen & Shephard (2004).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            returns = self.data['Returns'].values\n",
    "            window = min(window, len(returns) // 4)\n",
    "            window = max(window, 5)\n",
    "            \n",
    "            # Calculate bipower variation\n",
    "            abs_returns = np.abs(returns)\n",
    "            bipower_var = np.zeros(len(returns))\n",
    "            \n",
    "            for i in range(window, len(returns)):\n",
    "                # Bipower variation estimator\n",
    "                abs_ret_window = abs_returns[i-window+1:i+1]\n",
    "                if len(abs_ret_window) >= 2:\n",
    "                    bipower_var[i] = np.pi/2 * np.sum(abs_ret_window[:-1] * abs_ret_window[1:]) / window\n",
    "            \n",
    "            # Realized variance\n",
    "            realized_var = np.array([np.sum(returns[max(0, i-window+1):i+1]**2) \n",
    "                                    for i in range(len(returns))])\n",
    "            \n",
    "            # Jump component\n",
    "            jump_component = realized_var - bipower_var\n",
    "            jump_component = np.maximum(jump_component, 0)  # Ensure non-negative\n",
    "            \n",
    "            # Statistical test for jumps\n",
    "            test_stat = jump_component / np.sqrt(bipower_var + 1e-8)\n",
    "            critical_value = stats.norm.ppf(1 - significance_level)\n",
    "            \n",
    "            jump_anomalies = test_stat > critical_value\n",
    "            \n",
    "            self.anomalies['jumps'] = {\n",
    "                'anomalies': pd.Series(jump_anomalies, index=self.data.index),\n",
    "                'test_statistic': pd.Series(test_stat, index=self.data.index),\n",
    "                'jump_component': pd.Series(jump_component, index=self.data.index)\n",
    "            }\n",
    "            \n",
    "            return pd.Series(jump_anomalies, index=self.data.index)\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.warning(f\"Error in jump detection: {str(e)}\")\n",
    "            return pd.Series(False, index=self.data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c3f1646-7134-41a7-8bad-f7b41b605b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_multivariate_outliers(self, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Multivariate Anomaly Detection using Isolation Forest\n",
    "        \n",
    "    Theory: Isolation Forest works by randomly selecting features and split values,\n",
    "    isolating anomalies which require fewer splits.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Select features for multivariate analysis\n",
    "        feature_columns = ['Returns', 'Volatility', 'Rolling_Skewness', 'Rolling_Kurtosis', 'RSI']\n",
    "            \n",
    "        # Add volume-based features if available\n",
    "        if 'Volume_Weighted_Returns' in self.data.columns:\n",
    "            feature_columns.append('Volume_Weighted_Returns')\n",
    "            \n",
    "        # Filter available columns\n",
    "        available_features = [col for col in feature_columns if col in self.data.columns]\n",
    "        features = self.data[available_features].dropna()\n",
    "            \n",
    "        if len(features) < 10:\n",
    "            st.warning(\"Insufficient data for multivariate analysis\")\n",
    "            return pd.Series(False, index=self.data.index)\n",
    "            \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "            \n",
    "        # Fit Isolation Forest\n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_estimators=100\n",
    "        )\n",
    "            \n",
    "        anomaly_labels = iso_forest.fit_predict(features_scaled)\n",
    "        anomalies = anomaly_labels == -1\n",
    "            \n",
    "        # Calculate anomaly scores\n",
    "        anomaly_scores = iso_forest.decision_function(features_scaled)\n",
    "            \n",
    "        # Create full-length series\n",
    "        full_anomalies = pd.Series(False, index=self.data.index)\n",
    "        full_scores = pd.Series(0.0, index=self.data.index)\n",
    "            \n",
    "        full_anomalies.loc[features.index] = anomalies\n",
    "        full_scores.loc[features.index] = anomaly_scores\n",
    "            \n",
    "        self.anomalies['multivariate'] = {\n",
    "            'anomalies': full_anomalies,\n",
    "            'scores': full_scores,\n",
    "            'features_used': available_features\n",
    "        }\n",
    "            \n",
    "        return full_anomalies\n",
    "            \n",
    "    except Exception as e:\n",
    "        st.warning(f\"Error in multivariate outlier detection: {str(e)}\")\n",
    "        return pd.Series(False, index=self.data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f7cfd18-d06e-4d99-b555-27b4e66ba9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_regime_changes(self, window=60, threshold=2.5):\n",
    "    \"\"\"\n",
    "    Regime Change Detection using Rolling Statistics\n",
    "        \n",
    "    Theory: Financial markets experience regime changes - periods with\n",
    "    different statistical properties.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        window = min(window, len(self.data) // 3)\n",
    "        window = max(window, 10)\n",
    "            \n",
    "        # Calculate rolling statistics\n",
    "        rolling_mean = self.data['Returns'].rolling(window=window, min_periods=window//2).mean()\n",
    "        rolling_std = self.data['Returns'].rolling(window=window, min_periods=window//2).std()\n",
    "        rolling_skew = self.data['Returns'].rolling(window=window, min_periods=window//2).skew()\n",
    "        rolling_kurt = self.data['Returns'].rolling(window=window, min_periods=window//2).kurt()\n",
    "            \n",
    "        # Detect significant changes in statistics\n",
    "        mean_change = np.abs(rolling_mean.diff()) > threshold * (rolling_std + 1e-10)\n",
    "        vol_change = np.abs(rolling_std.pct_change()) > threshold * 0.1\n",
    "        skew_change = np.abs(rolling_skew.diff()) > threshold * 0.5\n",
    "        kurt_change = np.abs(rolling_kurt.diff()) > threshold * 1.0\n",
    "            \n",
    "        # Fill NaN values with False\n",
    "        mean_change = mean_change.fillna(False)\n",
    "        vol_change = vol_change.fillna(False)\n",
    "        skew_change = skew_change.fillna(False)\n",
    "        kurt_change = kurt_change.fillna(False)\n",
    "            \n",
    "        # Regime change when multiple statistics change\n",
    "        regime_changes = (mean_change.astype(int) + vol_change.astype(int) + \n",
    "                        skew_change.astype(int) + kurt_change.astype(int)) >= 2\n",
    "            \n",
    "        self.anomalies['regime'] = {\n",
    "            'anomalies': regime_changes,\n",
    "            'mean_change': mean_change,\n",
    "            'vol_change': vol_change,\n",
    "            'skew_change': skew_change,\n",
    "            'kurt_change': kurt_change\n",
    "        }\n",
    "            \n",
    "        return regime_changes\n",
    "            \n",
    "    except Exception as e:\n",
    "        st.warning(f\"Error in regime change detection: {str(e)}\")\n",
    "        return pd.Series(False, index=self.data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e535b4f-2e5a-4928-a198-aa217f260045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomaly_summary(self):\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary of all detected anomalies\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "        \n",
    "    for method, results in self.anomalies.items():\n",
    "        if 'anomalies' in results:\n",
    "            anomalies = results['anomalies']\n",
    "            try:\n",
    "                total_anomalies = int(anomalies.sum()) if hasattr(anomalies, 'sum') else sum(anomalies)\n",
    "                anomaly_rate = (total_anomalies / len(anomalies) * 100) if len(anomalies) > 0 else 0\n",
    "                    \n",
    "                summary[method] = {\n",
    "                    'total_anomalies': total_anomalies,\n",
    "                    'anomaly_rate': round(anomaly_rate, 2),\n",
    "                    'first_anomaly': anomalies.index[anomalies].min() if hasattr(anomalies, 'index') and total_anomalies > 0 else None,\n",
    "                    'last_anomaly': anomalies.index[anomalies].max() if hasattr(anomalies, 'index') and total_anomalies > 0 else None\n",
    "                }\n",
    "            except Exception as e:\n",
    "                st.warning(f\"Error summarizing {method}: {str(e)}\")\n",
    "        \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f1420da-b56c-410c-b268-0f8f1ab8ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(detector, data):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for anomaly detection results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            subplot_titles=['Price with Anomalies', 'Returns with Statistical Outliers', \n",
    "                           'Volatility Clustering', 'Jump Detection'],\n",
    "            vertical_spacing=0.08,\n",
    "            specs=[[{\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        # Price chart with all anomalies\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=data.index, y=data['Close'], name='Price', line=dict(color='blue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add anomaly markers for each method\n",
    "        colors = {'statistical': 'red', 'volatility': 'orange', 'jumps': 'purple', \n",
    "                  'multivariate': 'green', 'regime': 'brown'}\n",
    "        \n",
    "        for method, color in colors.items():\n",
    "            if method in detector.anomalies:\n",
    "                anomalies = detector.anomalies[method]['anomalies']\n",
    "                if hasattr(anomalies, 'index'):\n",
    "                    anomaly_dates = anomalies.index[anomalies]\n",
    "                    if len(anomaly_dates) > 0:\n",
    "                        try:\n",
    "                            anomaly_prices = data.loc[anomaly_dates, 'Close']\n",
    "                            fig.add_trace(\n",
    "                                go.Scatter(x=anomaly_dates, y=anomaly_prices, \n",
    "                                         mode='markers', name=f'{method.title()} Anomalies',\n",
    "                                         marker=dict(color=color, size=8, symbol='diamond')),\n",
    "                                row=1, col=1\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            st.warning(f\"Could not plot {method} anomalies: {str(e)}\")\n",
    "        \n",
    "        # Returns with statistical outliers\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=data.index, y=data['Returns'], name='Returns', \n",
    "                      line=dict(color='darkblue')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Add statistical outlier markers\n",
    "        if 'statistical' in detector.anomalies:\n",
    "            stat_anomalies = detector.anomalies['statistical']['anomalies']\n",
    "            if hasattr(stat_anomalies, 'index'):\n",
    "                anomaly_indices = stat_anomalies.index[stat_anomalies]\n",
    "                if len(anomaly_indices) > 0:\n",
    "                    try:\n",
    "                        anomaly_returns = data.loc[anomaly_indices, 'Returns']\n",
    "                        fig.add_trace(\n",
    "                            go.Scatter(x=anomaly_indices, y=anomaly_returns,\n",
    "                                     mode='markers', name='Statistical Outliers',\n",
    "                                     marker=dict(color='red', size=6)),\n",
    "                            row=2, col=1\n",
    "                        )\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # Volatility clustering\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=data.index, y=data['Volatility'], name='Volatility',\n",
    "                      line=dict(color='green')),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        # Jump detection\n",
    "        if 'jumps' in detector.anomalies:\n",
    "            try:\n",
    "                jump_component = detector.anomalies['jumps']['jump_component']\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=jump_component.index, y=jump_component, \n",
    "                              name='Jump Component', line=dict(color='purple')),\n",
    "                    row=4, col=1\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        fig.update_layout(height=1200, title='Comprehensive Market Anomaly Detection')\n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"Error creating visualizations: {str(e)}\")\n",
    "        return go.Figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
