{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f21f27-9a96-4c8c-ad6a-26b866766535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Try to import yfinance with error handling\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    YFINANCE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    YFINANCE_AVAILABLE = False\n",
    "    st.error(\"yfinance is not installed. Please install it using: pip install yfinance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0b0261-2349-4d5a-aad8-692f035355c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Advanced Market Anomaly Detection System\n",
    "    \n",
    "    This class implements multiple statistical and machine learning methods\n",
    "    for detecting unusual events in financial time series data.\n",
    "    \n",
    "    Methods include:\n",
    "    1. Statistical Process Control (SPC) using control charts\n",
    "    2. Z-Score based outlier detection with rolling statistics\n",
    "    3. Isolation Forest for multivariate anomaly detection\n",
    "    4. Volatility clustering detection using GARCH-like approach\n",
    "    5. Jump detection using Bipower Variation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the detector with price data\n",
    "        \n",
    "         Args:\n",
    "            data (pd.DataFrame): DataFrame with OHLCV data and datetime index\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.returns = None\n",
    "        self.features = None\n",
    "        self.anomalies = {}\n",
    "        self._prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351b3fdd-ede2-4166-a22e-9809f6d3230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the detector with price data\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): DataFrame with OHLCV data and datetime index\n",
    "        \"\"\"\n",
    "        if data is None or data.empty:\n",
    "            raise ValueError(\"Input data is empty or None\")\n",
    "            \n",
    "        self.data = data.copy()\n",
    "        \n",
    "        # Ensure we have the required columns\n",
    "        if 'Close' not in self.data.columns:\n",
    "            # Try different column names\n",
    "            if 'close' in self.data.columns:\n",
    "                self.data['Close'] = self.data['close']\n",
    "            elif 'Adj Close' in self.data.columns:\n",
    "                self.data['Close'] = self.data['Adj Close']\n",
    "            else:\n",
    "                raise ValueError(\"No 'Close' price column found in data\")\n",
    "        \n",
    "        self.returns = None\n",
    "        self.features = None\n",
    "        self.anomalies = {}\n",
    "        self._prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43ad052-0924-462c-9c67-30850974cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    " def _prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepare financial features for analysis\n",
    "        \n",
    "        Theory: Financial returns are more stationary than prices and exhibit\n",
    "        properties like volatility clustering, fat tails, and mean reversion.\n",
    "        We compute multiple features to capture different aspects of market behavior.\n",
    "        \"\"\"\n",
    "        # Calculate returns\n",
    "        self.data['Returns'] = self.data['Close'].pct_change()\n",
    "        \n",
    "        # Log returns (more theoretically sound for continuous compounding)\n",
    "        self.data['Log_Returns'] = np.log(self.data['Close'] / self.data['Close'].shift(1))\n",
    "        \n",
    "        # Realized volatility (rolling standard deviation)\n",
    "        self.data['Volatility'] = self.data['Returns'].rolling(window=20).std()\n",
    "        \n",
    "        # Volume-weighted returns (if volume available)\n",
    "        if 'Volume' in self.data.columns:\n",
    "            self.data['Volume_Weighted_Returns'] = self.data['Returns'] * np.log(self.data['Volume'])\n",
    "        \n",
    "        # Higher moments for fat-tail analysis\n",
    "        self.data['Returns_Squared'] = self.data['Returns'] ** 2\n",
    "        self.data['Returns_Cubed'] = self.data['Returns'] ** 3\n",
    "        \n",
    "        # Rolling skewness and kurtosis (20-day window)\n",
    "        self.data['Rolling_Skewness'] = self.data['Returns'].rolling(window=20).skew()\n",
    "        self.data['Rolling_Kurtosis'] = self.data['Returns'].rolling(window=20).kurt()\n",
    "        \n",
    "        # Price momentum indicators\n",
    "        self.data['RSI'] = self._calculate_rsi(self.data['Close'], window=14)\n",
    "        \n",
    "        # Drop NaN values\n",
    "        self.data = self.data.dropna()\n",
    "        self.returns = self.data['Returns'].values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9e7c987-06f9-408c-8cc8-9fa3e3fe0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_rsi(self, prices, window=14):\n",
    "        \"\"\"\n",
    "        Calculate Relative Strength Index\n",
    "        \n",
    "        Theory: RSI measures momentum and can indicate overbought/oversold conditions\n",
    "        which often precede market anomalies or reversals.\n",
    "        \"\"\"\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1694f237-0a5e-476d-a95c-74a3cd4f8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_statistical_outliers(self, window=60, threshold=3):\n",
    "        \"\"\"\n",
    "        Statistical Process Control using rolling Z-scores\n",
    "        \n",
    "        Theory: Assumes returns follow a normal distribution locally.\n",
    "        Points beyond threshold standard deviations are flagged as anomalies.\n",
    "        \n",
    "        Args:\n",
    "            window (int): Rolling window for calculating statistics\n",
    "            threshold (float): Z-score threshold for anomaly detection\n",
    "            \n",
    "        Returns:\n",
    "            pd.Series: Boolean series indicating anomalies\n",
    "        \"\"\"\n",
    "        # Rolling mean and standard deviation\n",
    "        rolling_mean = self.data['Returns'].rolling(window=window).mean()\n",
    "        rolling_std = self.data['Returns'].rolling(window=window).std()\n",
    "        \n",
    "        # Calculate Z-scores\n",
    "        z_scores = (self.data['Returns'] - rolling_mean) / rolling_std\n",
    "        \n",
    "        # Detect anomalies\n",
    "        anomalies = np.abs(z_scores) > threshold\n",
    "        \n",
    "        self.anomalies['statistical'] = {\n",
    "            'anomalies': anomalies,\n",
    "            'z_scores': z_scores,\n",
    "            'threshold': threshold\n",
    "        }\n",
    "        \n",
    "        return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a59c728-9b31-4499-a7e5-cbbe1c2c4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_volatility_clusters(self, window=20, threshold=2):\n",
    "        \"\"\"\n",
    "        Volatility Clustering Detection\n",
    "        \n",
    "        Theory: Financial markets exhibit volatility clustering - periods of high\n",
    "        volatility tend to be followed by high volatility (ARCH/GARCH effects).\n",
    "        We detect when realized volatility exceeds historical norms.\n",
    "        \n",
    "        Args:\n",
    "            window (int): Window for calculating volatility statistics\n",
    "            threshold (float): Threshold in standard deviations\n",
    "            \n",
    "        Returns:\n",
    "            pd.Series: Boolean series indicating volatility anomalies\n",
    "        \"\"\"\n",
    "        # Calculate rolling volatility statistics\n",
    "        vol_mean = self.data['Volatility'].rolling(window=window).mean()\n",
    "        vol_std = self.data['Volatility'].rolling(window=window).std()\n",
    "        \n",
    "        # Standardize current volatility\n",
    "        vol_z_score = (self.data['Volatility'] - vol_mean) / vol_std\n",
    "        \n",
    "        # Detect volatility spikes\n",
    "        vol_anomalies = vol_z_score > threshold\n",
    "        \n",
    "        self.anomalies['volatility'] = {\n",
    "            'anomalies': vol_anomalies,\n",
    "            'z_scores': vol_z_score,\n",
    "            'threshold': threshold\n",
    "        }\n",
    "        \n",
    "        return vol_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85065c3e-3a65-400a-9173-7663ee839a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    " def detect_jump_diffusion(self, window=20, significance_level=0.01):\n",
    "        \"\"\"\n",
    "        Jump Detection using Bipower Variation\n",
    "        \n",
    "        Theory: Distinguishes between continuous price movements (diffusion)\n",
    "        and discontinuous jumps. Based on Barndorff-Nielsen & Shephard (2004).\n",
    "        Uses bipower variation to estimate continuous component of quadratic variation.\n",
    "        \n",
    "        Args:\n",
    "            window (int): Window for calculating bipower variation\n",
    "            significance_level (float): Statistical significance level\n",
    "            \n",
    "        Returns:\n",
    "            pd.Series: Boolean series indicating jump events\n",
    "        \"\"\"\n",
    "        returns = self.data['Returns'].values\n",
    "        \n",
    "        # Calculate bipower variation\n",
    "        abs_returns = np.abs(returns)\n",
    "        bipower_var = np.zeros(len(returns))\n",
    "        \n",
    "        for i in range(window, len(returns)):\n",
    "            # Bipower variation estimator\n",
    "            abs_ret_window = abs_returns[i-window+1:i+1]\n",
    "            if len(abs_ret_window) >= 2:\n",
    "                bipower_var[i] = np.pi/2 * np.sum(abs_ret_window[:-1] * abs_ret_window[1:]) / window\n",
    "        \n",
    "        # Realized variance\n",
    "        realized_var = np.array([np.sum(returns[max(0, i-window+1):i+1]**2) \n",
    "                                for i in range(len(returns))])\n",
    "        \n",
    "        # Jump component\n",
    "        jump_component = realized_var - bipower_var\n",
    "        jump_component = np.maximum(jump_component, 0)  # Ensure non-negative\n",
    "        \n",
    "        # Statistical test for jumps\n",
    "        # Using asymptotic distribution theory\n",
    "        test_stat = jump_component / np.sqrt(bipower_var + 1e-8)  # Add small epsilon\n",
    "        critical_value = stats.norm.ppf(1 - significance_level)\n",
    "        \n",
    "        jump_anomalies = test_stat > critical_value\n",
    "        \n",
    "        self.anomalies['jumps'] = {\n",
    "            'anomalies': pd.Series(jump_anomalies, index=self.data.index),\n",
    "            'test_statistic': pd.Series(test_stat, index=self.data.index),\n",
    "            'jump_component': pd.Series(jump_component, index=self.data.index)\n",
    "        }\n",
    "        \n",
    "        return pd.Series(jump_anomalies, index=self.data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3f1646-7134-41a7-8bad-f7b41b605b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_multivariate_outliers(self, contamination=0.05):\n",
    "        \"\"\"\n",
    "        Multivariate Anomaly Detection using Isolation Forest\n",
    "        \n",
    "        Theory: Isolation Forest works by randomly selecting features and split values,\n",
    "        isolating anomalies which require fewer splits. Effective for high-dimensional\n",
    "        financial data where multiple indicators may signal anomalies simultaneously.\n",
    "        \n",
    "        Args:\n",
    "            contamination (float): Expected proportion of anomalies\n",
    "            \n",
    "        Returns:\n",
    "            pd.Series: Boolean series indicating multivariate anomalies\n",
    "        \"\"\"\n",
    "        # Select features for multivariate analysis\n",
    "        feature_columns = ['Returns', 'Volatility', 'Rolling_Skewness', \n",
    "                          'Rolling_Kurtosis', 'RSI']\n",
    "        \n",
    "        # Add volume-based features if available\n",
    "        if 'Volume_Weighted_Returns' in self.data.columns:\n",
    "            feature_columns.append('Volume_Weighted_Returns')\n",
    "        \n",
    "        features = self.data[feature_columns].dropna()\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        \n",
    "        # Fit Isolation Forest\n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_estimators=100\n",
    "        )\n",
    "        \n",
    "        anomaly_labels = iso_forest.fit_predict(features_scaled)\n",
    "        anomalies = anomaly_labels == -1\n",
    "        \n",
    "        # Calculate anomaly scores\n",
    "        anomaly_scores = iso_forest.decision_function(features_scaled)\n",
    "        \n",
    "        self.anomalies['multivariate'] = {\n",
    "            'anomalies': pd.Series(anomalies, index=features.index),\n",
    "            'scores': pd.Series(anomaly_scores, index=features.index),\n",
    "            'features_used': feature_columns\n",
    "        }\n",
    "        \n",
    "        return pd.Series(anomalies, index=features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f7cfd18-d06e-4d99-b555-27b4e66ba9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_regime_changes(self, window=60, threshold=2.5):\n",
    "        \"\"\"\n",
    "        Regime Change Detection using Rolling Statistics\n",
    "        \n",
    "        Theory: Financial markets experience regime changes - periods with\n",
    "        different statistical properties. We detect when multiple moments\n",
    "        of the return distribution change simultaneously.\n",
    "        \n",
    "        Args:\n",
    "            window (int): Window for calculating regime statistics\n",
    "            threshold (float): Threshold for detecting significant changes\n",
    "            \n",
    "        Returns:\n",
    "            pd.Series: Boolean series indicating regime changes\n",
    "        \"\"\"\n",
    "        # Calculate rolling statistics\n",
    "        rolling_mean = self.data['Returns'].rolling(window=window).mean()\n",
    "        rolling_std = self.data['Returns'].rolling(window=window).std()\n",
    "        rolling_skew = self.data['Returns'].rolling(window=window).skew()\n",
    "        rolling_kurt = self.data['Returns'].rolling(window=window).kurt()\n",
    "        \n",
    "        # Detect significant changes in statistics\n",
    "        mean_change = np.abs(rolling_mean.diff()) > threshold * rolling_std\n",
    "        vol_change = np.abs(rolling_std.pct_change()) > threshold * 0.1\n",
    "        skew_change = np.abs(rolling_skew.diff()) > threshold * 0.5\n",
    "        kurt_change = np.abs(rolling_kurt.diff()) > threshold * 1.0\n",
    "        \n",
    "        # Regime change when multiple statistics change\n",
    "        regime_changes = (mean_change.astype(int) + vol_change.astype(int) + \n",
    "                         skew_change.astype(int) + kurt_change.astype(int)) >= 2\n",
    "        \n",
    "        self.anomalies['regime'] = {\n",
    "            'anomalies': regime_changes,\n",
    "            'mean_change': mean_change,\n",
    "            'vol_change': vol_change,\n",
    "            'skew_change': skew_change,\n",
    "            'kurt_change': kurt_change\n",
    "        }\n",
    "        \n",
    "        return regime_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e535b4f-2e5a-4928-a198-aa217f260045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomaly_summary(self):\n",
    "        \"\"\"\n",
    "        Generate comprehensive summary of all detected anomalies\n",
    "        \n",
    "        Returns:\n",
    "            dict: Summary statistics for each detection method\n",
    "        \"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        for method, results in self.anomalies.items():\n",
    "            if 'anomalies' in results:\n",
    "                anomalies = results['anomalies']\n",
    "                summary[method] = {\n",
    "                    'total_anomalies': anomalies.sum() if hasattr(anomalies, 'sum') else sum(anomalies),\n",
    "                    'anomaly_rate': (anomalies.sum() / len(anomalies) * 100) if hasattr(anomalies, 'sum') else (sum(anomalies) / len(anomalies) * 100),\n",
    "                    'first_anomaly': anomalies.index[anomalies].min() if hasattr(anomalies, 'index') and anomalies.sum() > 0 else None,\n",
    "                    'last_anomaly': anomalies.index[anomalies].max() if hasattr(anomalies, 'index') and anomalies.sum() > 0 else None\n",
    "                }\n",
    "        \n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f1420da-b56c-410c-b268-0f8f1ab8ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(detector, data):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for anomaly detection results\n",
    "    \n",
    "    Args:\n",
    "        detector: MarketAnomalyDetector instance\n",
    "        data: Original price data\n",
    "        \n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: Interactive plot with all anomalies\n",
    "    \"\"\"\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=1,\n",
    "        subplot_titles=['Price with Anomalies', 'Returns with Statistical Outliers', \n",
    "                       'Volatility Clustering', 'Jump Detection'],\n",
    "        vertical_spacing=0.08,\n",
    "        specs=[[{\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": True}],\n",
    "               [{\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Price chart with all anomalies\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=data.index, y=data['Close'], name='Price', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add anomaly markers for each method\n",
    "    colors = {'statistical': 'red', 'volatility': 'orange', 'jumps': 'purple', \n",
    "              'multivariate': 'green', 'regime': 'brown'}\n",
    "    \n",
    "    for method, color in colors.items():\n",
    "        if method in detector.anomalies:\n",
    "            anomalies = detector.anomalies[method]['anomalies']\n",
    "            if hasattr(anomalies, 'index'):\n",
    "                anomaly_dates = anomalies.index[anomalies]\n",
    "                anomaly_prices = data.loc[anomaly_dates, 'Close']\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=anomaly_dates, y=anomaly_prices, \n",
    "                             mode='markers', name=f'{method.title()} Anomalies',\n",
    "                             marker=dict(color=color, size=8, symbol='diamond')),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "    \n",
    "    # Returns with statistical outliers\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=data.index, y=data['Returns'], name='Returns', \n",
    "                  line=dict(color='darkblue')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    if 'statistical' in detector.anomalies:\n",
    "        stat_anomalies = detector.anomalies['statistical']['anomalies']\n",
    "        if hasattr(stat_anomalies, 'index'):\n",
    "            anomaly_returns = data.loc[stat_anomalies.index[stat_anomalies], 'Returns']\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=stat_anomalies.index[stat_anomalies], y=anomaly_returns,\n",
    "                         mode='markers', name='Statistical Outliers',\n",
    "                         marker=dict(color='red', size=6)),\n",
    "                row=2, col=1\n",
    "            )\n",
    "    \n",
    "    # Volatility clustering\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=data.index, y=data['Volatility'], name='Volatility',\n",
    "                  line=dict(color='green')),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    if 'volatility' in detector.anomalies:\n",
    "        vol_anomalies = detector.anomalies['volatility']['anomalies']\n",
    "        if hasattr(vol_anomalies, 'index'):\n",
    "            anomaly_vol = data.loc[vol_anomalies.index[vol_anomalies], 'Volatility']\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=vol_anomalies.index[vol_anomalies], y=anomaly_vol,\n",
    "                         mode='markers', name='Volatility Spikes',\n",
    "                         marker=dict(color='orange', size=6)),\n",
    "                row=3, col=1\n",
    "            )\n",
    "    \n",
    "    # Jump detection\n",
    "    if 'jumps' in detector.anomalies:\n",
    "        jump_component = detector.anomalies['jumps']['jump_component']\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=jump_component.index, y=jump_component, \n",
    "                      name='Jump Component', line=dict(color='purple')),\n",
    "            row=4, col=1\n",
    "        )\n",
    "        \n",
    "        jump_anomalies = detector.anomalies['jumps']['anomalies']\n",
    "        anomaly_jumps = jump_component[jump_anomalies]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=anomaly_jumps.index, y=anomaly_jumps,\n",
    "                     mode='markers', name='Detected Jumps',\n",
    "                     marker=dict(color='red', size=6)),\n",
    "            row=4, col=1\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=1200, title='Comprehensive Market Anomaly Detection')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b7c3a-b861-4b00-8aa0-4656396dcdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
